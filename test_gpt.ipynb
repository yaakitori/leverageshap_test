{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8d2190",
   "metadata": {},
   "source": [
    "# Shapley 値推定精度比較実験（Kernel SHAP・Optimized Kernel SHAP・Leverage SHAP）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a9806",
   "metadata": {},
   "source": [
    "インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146025c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap  # データセット読み込み & Optimized Kernel SHAP\n",
    "import xgboost as xgb  # 決定木モデル（Tree SHAP 用）\n",
    "\n",
    "np.random.seed(0)  # 再現性確保\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a410b5",
   "metadata": {},
   "source": [
    "データセット読込 → XGBoost 学習 → 真の Shapley 値取得 (Tree SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80746e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset: features = 4, training samples = 149.\n",
      "California dataset: features = 8, training samples = 999.\n",
      "Diabetes dataset: features = 10, training samples = 441.\n",
      "Adult dataset: features = 12, training samples = 999.\n",
      "Correlated dataset: features = 60, training samples = 999.\n",
      "Independent dataset: features = 60, training samples = 999.\n",
      "Nhanes dataset: features = 79, training samples = 999.\n",
      "Communities dataset: features = 102, training samples = 999.\n"
     ]
    }
   ],
   "source": [
    "# Prepare a dictionary to hold dataset info and results\n",
    "datasets_info = {\n",
    "    \"iris\": {\"loader\": shap.datasets.iris, \"n_points\": None, \"task\": \"classification\"},\n",
    "    \"california\": {\"loader\": shap.datasets.california, \"n_points\": 1000, \"task\": \"regression\"},\n",
    "    \"diabetes\": {\"loader\": shap.datasets.diabetes, \"n_points\": None, \"task\": \"regression\"},\n",
    "    \"adult\": {\"loader\": shap.datasets.adult, \"n_points\": 1000, \"task\": \"classification\"},\n",
    "    \"correlated\": {\"loader\": shap.datasets.corrgroups60, \"n_points\": 1000, \"task\": \"regression\"},\n",
    "    \"independent\": {\"loader\": shap.datasets.independentlinear60, \"n_points\": 1000, \"task\": \"regression\"},\n",
    "    \"nhanes\": {\"loader\": shap.datasets.nhanesi, \"n_points\": 1000, \"task\": \"regression\"},\n",
    "    \"communities\": {\"loader\": shap.datasets.communitiesandcrime, \"n_points\": 1000, \"task\": \"regression\"},\n",
    "}\n",
    "\n",
    "# Storage for ground truth Shapley values and models\n",
    "ground_truth_phi = {}\n",
    "models = {}\n",
    "\n",
    "X_full = {}  # 全行\n",
    "x_exp_dict = {}  # 説明対象の 1 行（Series）\n",
    "\n",
    "for name, info in datasets_info.items():\n",
    "    # Load dataset\n",
    "    if info[\"n_points\"] is not None:\n",
    "        X, y = info[\"loader\"](n_points=info[\"n_points\"])\n",
    "    else:\n",
    "        X, y = info[\"loader\"]()  # load full dataset if n_points not specified\n",
    "    # Ensure X is a DataFrame and y is an array or Series\n",
    "    X = pd.DataFrame(X)  # (some shap loaders return np.array, ensure DataFrame)\n",
    "    y = pd.Series(y)\n",
    "\n",
    "    # Use the first data point as the instance to explain\n",
    "    x_exp = X.iloc[0]\n",
    "    x_exp_dict[name] = X.iloc[0]   # ★ 保存\n",
    "    X_full[name] = X               # ★ 保存\n",
    "    X_train = X.iloc[1:]\n",
    "    y_train = y.iloc[1:]\n",
    "\n",
    "    # Train XGBoost model\n",
    "    if info[\"task\"] == \"classification\":\n",
    "        # Determine number of classes\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        if num_classes > 2:\n",
    "            model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "        else:\n",
    "            model = xgb.XGBClassifier(eval_metric=\"logloss\")\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    models[name] = model\n",
    "\n",
    "    # Compute ground truth Shapley values using Tree SHAP\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # Get shap values for the single instance (as a 1x n_features input)\n",
    "    if info[\"task\"] == \"classification\" and len(np.unique(y_train)) > 2:\n",
    "        # For multi-class, TreeExplainer.shap_values returns a list of arrays (one per class)\n",
    "        phi = explainer.shap_values(pd.DataFrame([x_exp]))\n",
    "        # We take the Shapley values for the predicted class of this instance\n",
    "        pred_class = model.predict(pd.DataFrame([x_exp]))[0]\n",
    "        phi = phi[pred_class]  # shape (n_features,)\n",
    "    else:\n",
    "        # For regression or binary classification (TreeExplainer returns one array for binary)\n",
    "        phi = explainer.shap_values(pd.DataFrame([x_exp]))[0]\n",
    "    ground_truth_phi[name] = np.array(phi)\n",
    "    print(f\"{name.capitalize()} dataset: features = {X.shape[1]}, training samples = {X_train.shape[0]}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca16cc",
   "metadata": {},
   "source": [
    " Kernel SHAP (Monte Carlo sampling + weighted regression)の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b715d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_shap_estimate(baseline, explicand, model_fn, num_samples):\n",
    "    \"\"\"\n",
    "    Estimate Shapley values using Kernel SHAP (Monte Carlo sampling + weighted regression).\n",
    "    :param baseline: 1D numpy array of baseline feature values (for \"missing\" features).\n",
    "    :param explicand: 1D numpy array of the instance's feature values (to explain).\n",
    "    :param model_fn: function that takes a 2D array of inputs and returns model outputs.\n",
    "                     For classification, model_fn should return the raw prediction (log-odds or probability) for the class of interest.\n",
    "    :param num_samples: number of subset samples to draw (excluding the empty and full subsets which are always used).\n",
    "    :return: numpy array of estimated Shapley values (length = number of features).\n",
    "    \"\"\"\n",
    "    n = explicand.shape[0]\n",
    "    # Prepare arrays for design matrix and target values\n",
    "    X_mat = []\n",
    "    y_vec = []\n",
    "    weights = []\n",
    "    \n",
    "    # Ensure empty set and full set are included\n",
    "    empty_mask = np.zeros(n, dtype=int)\n",
    "    full_mask = np.ones(n, dtype=int)\n",
    "    X_mat.append(empty_mask)\n",
    "    X_mat.append(full_mask)\n",
    "    # Model evaluations for empty and full\n",
    "    y_empty = model_fn(np.array([baseline]))[0]\n",
    "    y_full = model_fn(np.array([explicand]))[0]\n",
    "    y_vec.append(y_empty)\n",
    "    y_vec.append(y_full)\n",
    "    # Assign a very large weight to empty and full to treat them as constraints in the regression\n",
    "    weights.append(10**6)\n",
    "    weights.append(10**6)\n",
    "    \n",
    "    # Random generator\n",
    "    rng = np.random.default_rng()\n",
    "    for _ in range(num_samples):\n",
    "        mask = rng.integers(0, 2, size=n, endpoint=False)  # random 0/1 mask\n",
    "        if mask.sum() == 0 or mask.sum() == n:\n",
    "            continue  # skip if accidentally got all-0 or all-1 (already included)\n",
    "        # Prepare the input sample: baseline for 0 features, explicand for 1 features\n",
    "        sample = baseline.copy()\n",
    "        sample[mask == 1] = explicand[mask == 1]\n",
    "        # Evaluate model\n",
    "        y_val = model_fn(np.array([sample]))[0]\n",
    "        X_mat.append(mask.astype(int))\n",
    "        y_vec.append(y_val)\n",
    "        # Kernel weight for this subset\n",
    "        k = mask.sum()\n",
    "        # Weight formula: (n-1) / [C(n, k) * k * (n-k)] (Kernel SHAP weighting)\n",
    "        # Use math.comb for binomial coefficient\n",
    "        W = (n - 1) / ( (np.math.comb(n, k)) * k * (n - k) )\n",
    "        weights.append(W)\n",
    "    \n",
    "    X_mat = np.array(X_mat)  # shape (m_total, n)\n",
    "    y_vec = np.array(y_vec)\n",
    "    weights = np.array(weights)\n",
    "    \n",
    "    # Use weighted least squares to solve for phi (Shapley values).\n",
    "    # We center the outputs by subtracting the model output for empty set (which will serve as phi0).\n",
    "    phi0 = y_vec[0]  # y_empty\n",
    "    y_centered = y_vec - phi0\n",
    "    # Apply weights: multiply each row by sqrt(weight)\n",
    "    W_sqrt = np.sqrt(weights)\n",
    "    Xw = X_mat * W_sqrt[:, None]\n",
    "    yw = y_centered * W_sqrt\n",
    "    # Solve least squares\n",
    "    phi, *_ = np.linalg.lstsq(Xw, yw, rcond=None)\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6172e6b",
   "metadata": {},
   "source": [
    "Optimized Kernel SHAP (SHAP Library)の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81d5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_kernel_shap_estimate(background_data, explicand, model_fn, num_samples):\n",
    "    \"\"\"\n",
    "    Estimate Shapley values using SHAP's optimized KernelExplainer.\n",
    "    :param background_data: DataFrame or numpy array for background (reference) distribution of features.\n",
    "    :param explicand: 1D numpy array of the instance's feature values to explain.\n",
    "    :param model_fn: function that returns the model output of interest (should be vectorized).\n",
    "    :param num_samples: number of evaluations to use in KernelExplainer (nsamples parameter).\n",
    "    :return: numpy array of estimated Shapley values.\n",
    "    \"\"\"\n",
    "    explainer = shap.KernelExplainer(model_fn, background_data)\n",
    "    phi = explainer.shap_values(explicand, nsamples=num_samples)\n",
    "    phi = np.array(phi)\n",
    "    # KernelExplainer returns a single array for regression or binary classification,\n",
    "    # but returns a list of arrays for multi-class. We handle both:\n",
    "    if isinstance(phi, list):\n",
    "        # If it's multi-class, we assume model_fn was set to output only the class of interest,\n",
    "        # so shap_values returns a list of length 1.\n",
    "        phi = phi[0]\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5087ac37",
   "metadata": {},
   "source": [
    " Leverage SHAP (Leverage Score Sampling)を近似的に実装。（元のリポジトリの実装とは異なる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ade77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "\n",
    "\n",
    "def leverage_shap_estimate(baseline, explicand, model_fn, num_samples):\n",
    "    \"\"\"\n",
    "    Estimate Shapley values using Leverage SHAP (approximate leverage score sampling).\n",
    "    :param baseline: 1D numpy array of baseline feature values.\n",
    "    :param explicand: 1D numpy array of instance feature values.\n",
    "    :param model_fn: function that returns model output of interest.\n",
    "    :param num_samples: total number of subset samples to draw.\n",
    "    :return: numpy array of estimated Shapley values.\n",
    "    \"\"\"\n",
    "    n = explicand.shape[0]\n",
    "    X_mat = []\n",
    "    y_vec = []\n",
    "    weights = []\n",
    "    # Include empty and full as constraints (with high weight)\n",
    "    empty_mask = np.zeros(n, dtype=int)\n",
    "    full_mask = np.ones(n, dtype=int)\n",
    "    X_mat.append(empty_mask)\n",
    "    X_mat.append(full_mask)\n",
    "    y_empty = model_fn(np.array([baseline]))[0]\n",
    "    y_full = model_fn(np.array([explicand]))[0]\n",
    "    y_vec.append(y_empty)\n",
    "    y_vec.append(y_full)\n",
    "    weights.append(10**6)\n",
    "    weights.append(10**6)\n",
    "    # Determine how many subsets to sample for each size 1..n-1\n",
    "    size_probs = np.array([0 if (k == 0 or k == n) else 1 / (k * (n - k)) for k in range(n + 1)], dtype=float)\n",
    "    size_probs = size_probs / size_probs.sum()\n",
    "    # Draw number of samples for each subset size\n",
    "    samples_per_size = {k: 0 for k in range(1, n)}\n",
    "    remaining = num_samples\n",
    "    # Use multinomial draw for how many subsets of each size\n",
    "    draws = np.random.multinomial(num_samples, [size_probs[k] for k in range(n + 1)])\n",
    "    for k in range(1, n):\n",
    "        samples_per_size[k] = draws[k]\n",
    "    # Now sample subsets for each size without replacement\n",
    "    for k, m_k in samples_per_size.items():\n",
    "        if m_k <= 0:\n",
    "            continue\n",
    "        # All possible subsets of size k (could be large if n is big; we sample without replacement)\n",
    "        # If total combinations C(n, k) is less than m_k, we'll take all.\n",
    "        # Otherwise, randomly choose m_k subsets of size k.\n",
    "        if m_k >= math.comb(n, k):\n",
    "            subsets_k = list(itertools.combinations(range(n), k))\n",
    "        else:\n",
    "            subsets_k = list(itertools.combinations(range(n), k))\n",
    "            subsets_k = np.array(subsets_k, dtype=object)\n",
    "            # choose m_k unique subsets\n",
    "            subset_indices = np.random.choice(len(subsets_k), size=m_k, replace=False)\n",
    "            subsets_k = subsets_k[subset_indices]\n",
    "        for comb_idx in subsets_k:\n",
    "            mask = np.zeros(n, dtype=int)\n",
    "            mask[list(comb_idx)] = 1\n",
    "            sample = baseline.copy()\n",
    "            sample[mask == 1] = explicand[mask == 1]\n",
    "            y_val = model_fn(np.array([sample]))[0]\n",
    "            X_mat.append(mask)\n",
    "            y_vec.append(y_val)\n",
    "            # We use the same Kernel SHAP weight for consistency\n",
    "            k_size = mask.sum()\n",
    "            W = (n - 1) / ((math.comb(n, k_size)) * k_size * (n - k_size))\n",
    "            weights.append(W)\n",
    "    X_mat = np.array(X_mat)\n",
    "    y_vec = np.array(y_vec)\n",
    "    weights = np.array(weights)\n",
    "    # Solve weighted least squares (same as in kernel_shap_estimate)\n",
    "    phi0 = y_vec[0]\n",
    "    y_centered = y_vec - phi0\n",
    "    W_sqrt = np.sqrt(weights)\n",
    "    Xw = X_mat * W_sqrt[:, None]\n",
    "    yw = y_centered * W_sqrt\n",
    "    phi, *_ = np.linalg.lstsq(Xw, yw, rcond=None)\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2247ed",
   "metadata": {},
   "source": [
    "Run Experiments and Compare Shapley Value Estimation Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e998e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for sample sizes and trials\n",
    "sample_multiples = [5, 10, 20]  # correspond to 5n, 10n, 20n\n",
    "num_trials = 100  # number of random runs for each setting\n",
    "\n",
    "# Prepare a dataframe to collect median L2 errors\n",
    "results_df = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [datasets_info.keys(), [f\"{m}n\" for m in sample_multiples]]),\n",
    "    columns=[\"Kernel SHAP\", \"Optimized Kernel SHAP\", \"Leverage SHAP\"]\n",
    ")\n",
    "\n",
    "# Evaluate each dataset\n",
    "for name in datasets_info.keys():\n",
    "    # Prepare baseline vector as the mean of training data features (for missing feature values)\n",
    "    X_train = pd.DataFrame(models[name].get_booster().feature_names, columns=None)\n",
    "    # If the model doesn't keep feature_names, use the original X training data we stored:\n",
    "    # (We can retrieve baseline from X used earlier in ground_truth_phi calculation)\n",
    "    X_train = X_train if not X_train.empty else pd.DataFrame(ground_truth_phi[name]).T.iloc[0:0]  # fallback (adjust as needed)\n",
    "\n",
    "# We will use the training data as background for KernelExplainer and derive baseline from it\n",
    "baseline_vecs = {}\n",
    "background_data = {}\n",
    "\n",
    "# (Re-run the data loading loop to also store baseline and background data)\n",
    "for name, info in datasets_info.items():\n",
    "    if info[\"n_points\"] is not None:\n",
    "        X, y = info[\"loader\"](n_points=info[\"n_points\"])\n",
    "    else:\n",
    "        X, y = info[\"loader\"]()\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "    x_exp = X.iloc[0]  # instance to explain\n",
    "    X_train = X.iloc[1:]\n",
    "    y_train = y.iloc[1:]\n",
    "    # Train model (same as before)\n",
    "    if info[\"task\"] == \"classification\":\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        model = xgb.XGBClassifier(eval_metric=\"logloss\") if num_classes <= 2 else xgb.XGBClassifier(eval_metric=\"mlogloss\")\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model = xgb.XGBRegressor()\n",
    "        model.fit(X_train, y_train)\n",
    "    models[name] = model\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    if info[\"task\"] == \"classification\" and len(np.unique(y_train)) > 2:\n",
    "        phi_all = explainer.shap_values(pd.DataFrame([x_exp]))\n",
    "        pred_class = model.predict(pd.DataFrame([x_exp]))[0]\n",
    "        ground_truth_phi[name] = np.array(phi_all[pred_class])\n",
    "    else:\n",
    "        ground_truth_phi[name] = np.array(explainer.shap_values(pd.DataFrame([x_exp]))[0])\n",
    "    # Store baseline vector (mean of training features) and background data\n",
    "    baseline_vecs[name] = X_train.mean().values\n",
    "    background_data[name] = X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f301a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] ['0', '1', '2', '3']\nexpected petal length (cm), petal width (cm), sepal length (cm), sepal width (cm) in input data\ntraining data did not have the following fields: 3, 0, 1, 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m errors_leverage = []\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trials):\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Kernel SHAP (Monte Carlo)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     phi_kernel = \u001b[43mkernel_shap_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m     err_kernel = np.linalg.norm(phi_kernel - ground_truth_phi[name])\n\u001b[32m     43\u001b[39m     errors_kernel.append(err_kernel)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mkernel_shap_estimate\u001b[39m\u001b[34m(baseline, explicand, model_fn, num_samples)\u001b[39m\n\u001b[32m     21\u001b[39m X_mat.append(full_mask)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Model evaluations for empty and full\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m y_empty = \u001b[43mmodel_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m y_full = model_fn(np.array([explicand]))[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m y_vec.append(y_empty)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmodel_fn\u001b[39m\u001b[34m(X_batch, model, c)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_fn\u001b[39m(X_batch, model=models[name], c=pred_class):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     margin = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m margin[:, c]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\sklearn.py:1718\u001b[39m, in \u001b[36mXGBClassifier.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1707\u001b[39m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m   1709\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1715\u001b[39m     iteration_range: Optional[IterationRange] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1716\u001b[39m ) -> ArrayLike:\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity=\u001b[38;5;28mself\u001b[39m.verbosity):\n\u001b[32m-> \u001b[39m\u001b[32m1718\u001b[39m         class_probs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1719\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1723\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1725\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[32m   1726\u001b[39m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[32m   1727\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\sklearn.py:1327\u001b[39m, in \u001b[36mXGBModel.predict\u001b[39m\u001b[34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[39m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._can_use_inplace_predict():\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m         predts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmargin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[32m   1336\u001b[39m             cp = import_cupy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\core.py:2667\u001b[39m, in \u001b[36mBooster.inplace_predict\u001b[39m\u001b[34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[39m\n\u001b[32m   2665\u001b[39m     data, fns, _ = _transform_pandas_df(data, enable_categorical)\n\u001b[32m   2666\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[32m-> \u001b[39m\u001b[32m2667\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2668\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[32m   2669\u001b[39m     data = np.array(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shoei\\miniconda3\\envs\\levshap_env\\Lib\\site-packages\\xgboost\\core.py:3243\u001b[39m, in \u001b[36mBooster._validate_features\u001b[39m\u001b[34m(self, feature_names)\u001b[39m\n\u001b[32m   3237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[32m   3238\u001b[39m     msg += (\n\u001b[32m   3239\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mtraining data did not have the following fields: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3240\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[32m   3241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3243\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg.format(\u001b[38;5;28mself\u001b[39m.feature_names, feature_names))\n",
      "\u001b[31mValueError\u001b[39m: feature_names mismatch: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] ['0', '1', '2', '3']\nexpected petal length (cm), petal width (cm), sepal length (cm), sepal width (cm) in input data\ntraining data did not have the following fields: 3, 0, 1, 2"
     ]
    }
   ],
   "source": [
    "# Define sample size values in terms of multiples of n (number of features)\n",
    "sample_multiples = [5, 10, 20]\n",
    "num_trials = 30  # use 30 for demo (use 100 for full experiment)\n",
    "\n",
    "# Results storage: a multi-index DataFrame [dataset, m] x [method]\n",
    "results = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([datasets_info.keys(), [f\"{m}n\" for m in sample_multiples]]),\n",
    "    columns=[\"Kernel SHAP\", \"Optimized Kernel SHAP\", \"Leverage SHAP\"],\n",
    ")\n",
    "\n",
    "for name, info in datasets_info.items():\n",
    "    n = len(baseline_vecs[name])  # number of features\n",
    "    X_dataset = X_full[name]  # ★ データフレームを取り出す\n",
    "    x_exp = x_exp_dict[name]  # ★ 説明対象 1 行を取り出す\n",
    "    # Define model function for shap computations:\n",
    "    if info[\"task\"] == \"classification\" and models[name].n_classes_ > 2:\n",
    "        pred_class = models[name].predict(pd.DataFrame([x_exp]))[0]\n",
    "\n",
    "        def model_fn(X_batch, model=models[name], c=pred_class):\n",
    "            margin = model.predict(pd.DataFrame(X_batch), output_margin=True)\n",
    "            return margin[:, c]\n",
    "    elif info[\"task\"] == \"classification\":\n",
    "\n",
    "        def model_fn(X_batch, model=models[name]):\n",
    "            return model.predict(pd.DataFrame(X_batch), output_margin=True)\n",
    "    else:\n",
    "\n",
    "        def model_fn(X_batch, model=models[name]):\n",
    "            return model.predict(pd.DataFrame(X_batch))\n",
    "\n",
    "    baseline = baseline_vecs[name]\n",
    "    explicand = x_exp.values  # ★ 対象行の値\n",
    "\n",
    "    for m_mult in sample_multiples:\n",
    "        m_val = m_mult * n\n",
    "        errors_kernel = []\n",
    "        errors_opt = []\n",
    "        errors_leverage = []\n",
    "        for t in range(num_trials):\n",
    "            # Kernel SHAP (Monte Carlo)\n",
    "            phi_kernel = kernel_shap_estimate(baseline, explicand, model_fn, num_samples=m_val)\n",
    "            err_kernel = np.linalg.norm(phi_kernel - ground_truth_phi[name])\n",
    "            errors_kernel.append(err_kernel)\n",
    "            # Optimized Kernel SHAP (shap.KernelExplainer)\n",
    "            phi_opt = optimized_kernel_shap_estimate(background_data[name], explicand, model_fn, num_samples=m_val)\n",
    "            err_opt = np.linalg.norm(phi_opt - ground_truth_phi[name])\n",
    "            errors_opt.append(err_opt)\n",
    "            # Leverage SHAP\n",
    "            phi_leverage = leverage_shap_estimate(baseline, explicand, model_fn, num_samples=m_val)\n",
    "            err_lev = np.linalg.norm(phi_leverage - ground_truth_phi[name])\n",
    "            errors_leverage.append(err_lev)\n",
    "        # Compute median errors\n",
    "        median_kernel = float(np.median(errors_kernel))\n",
    "        median_opt = float(np.median(errors_opt))\n",
    "        median_leverage = float(np.median(errors_leverage))\n",
    "        results.loc[(name, f\"{m_mult}n\"), \"Kernel SHAP\"] = median_kernel\n",
    "        results.loc[(name, f\"{m_mult}n\"), \"Optimized Kernel SHAP\"] = median_opt\n",
    "        results.loc[(name, f\"{m_mult}n\"), \"Leverage SHAP\"] = median_leverage\n",
    "        print(\n",
    "            f\"{name.capitalize()} (m={m_mult}n): Kernel={median_kernel:.4f}, Optimized={median_opt:.4f}, Leverage={median_leverage:.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Iris dataset (n=4 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**California dataset (n=8 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Diabetes dataset (n=10 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Adult dataset (n=12 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Correlated dataset (n=60 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Independent dataset (n=60 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Nhanes dataset (n=79 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "\n",
      "**Communities dataset (n=102 features)**\n",
      "  m = 5n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 10n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n",
      "  m = 20n:  Kernel SHAP = nan,  Optimized Kernel SHAP = nan,  Leverage SHAP = nan\n"
     ]
    }
   ],
   "source": [
    "# Display the median L2 errors for each dataset and each method (for m = 5n, 10n, 20n)\n",
    "for name in datasets_info.keys():\n",
    "    print(f\"\\n**{name.capitalize()} dataset (n={len(baseline_vecs[name])} features)**\")\n",
    "    for m_mult in sample_multiples:\n",
    "        med_kernel = results.loc[(name, f\"{m_mult}n\"), \"Kernel SHAP\"]\n",
    "        med_opt = results.loc[(name, f\"{m_mult}n\"), \"Optimized Kernel SHAP\"]\n",
    "        med_lev = results.loc[(name, f\"{m_mult}n\"), \"Leverage SHAP\"]\n",
    "        print(\n",
    "            f\"  m = {m_mult}n:  Kernel SHAP = {med_kernel:.4f},  Optimized Kernel SHAP = {med_opt:.4f},  Leverage SHAP = {med_lev:.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1049c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "levshap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
